{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "import os\n",
    "\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "from imputers import OTimputer, RRimputer\n",
    "\n",
    "from utils import *\n",
    "from data_loaders import dataset_loader\n",
    "from softimpute import softimpute, cv_softimpute\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Ni</th>\n",
       "      <th>Mn</th>\n",
       "      <th>Co</th>\n",
       "      <th>Li</th>\n",
       "      <th>dapant</th>\n",
       "      <th>dopant_ratio</th>\n",
       "      <th>tem</th>\n",
       "      <th>holding_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>75</td>\n",
       "      <td>92.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>76</td>\n",
       "      <td>92.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>77</td>\n",
       "      <td>92.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>750</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>78</td>\n",
       "      <td>81.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>750</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>80</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>720</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id    Ni   Mn  Co   Li  dapant dopant_ratio  tem  holding_time\n",
       "0     1  95.0  0.0   4  101       0            1  800            10\n",
       "1     2  95.0  0.0   4  101      24            1  800            10\n",
       "2     3  95.0  0.0   4  101      21            1  800            10\n",
       "3     4  95.0  0.0   4  101      17            1  800            10\n",
       "4     5  95.0  0.0   4  101      16            1  800            10\n",
       "..   ..   ...  ...  ..  ...     ...          ...  ...           ...\n",
       "395  75  92.0  4.0   4  101       0            1  750            10\n",
       "396  76  92.0  4.0   4  101       0            3  750            10\n",
       "397  77  92.0  4.0   4  101       0            5  750            10\n",
       "398  78  81.5  0.0  15  105       0          3.5  750            15\n",
       "399  80  80.0  0.0  15  115       0            5  720            15\n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv(\"processed_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             400 non-null    int64  \n",
      " 1   Ni             400 non-null    float64\n",
      " 2   Mn             400 non-null    float64\n",
      " 3   Co             400 non-null    object \n",
      " 4   Li             400 non-null    int64  \n",
      " 5   dapant         400 non-null    int64  \n",
      " 6   dopant_ratio   400 non-null    object \n",
      " 7   tem            400 non-null    int64  \n",
      " 8    holding_time  400 non-null    object \n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 28.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95.0 0.0 '4' 101 0 '1' 800 '10']\n",
      " [95.0 0.0 '4' 101 24 '1' 800 '10']\n",
      " [95.0 0.0 '4' 101 21 '1' 800 '10']\n",
      " [95.0 0.0 '4' 101 17 '1' 800 '10']\n",
      " [95.0 0.0 '4' 101 16 '1' 800 '10']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. 去除 Id 列\n",
    "df = df.drop(\"Id\", axis=1)\n",
    "data = df.to_numpy()\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "列名: Ni\n",
      "唯一值: [95.  94.  99.  98.5 98. ]\n",
      "数据类型: float64\n",
      "\n",
      "列名: Mn\n",
      "唯一值: [ 0.    2.   10.    1.82  3.3 ]\n",
      "数据类型: float64\n",
      "\n",
      "列名: Co\n",
      "唯一值: ['4' '3' '0' '10' '2.9']\n",
      "数据类型: object\n",
      "\n",
      "列名: Li\n",
      "唯一值: [101 110 103 105 104]\n",
      "数据类型: int64\n",
      "\n",
      "列名: dapant\n",
      "唯一值: [ 0 24 21 17 16]\n",
      "数据类型: int64\n",
      "\n",
      "列名: dopant_ratio\n",
      "唯一值: ['1' '1.5' '2' '3' '4']\n",
      "数据类型: object\n",
      "\n",
      "列名: tem\n",
      "唯一值: [800 700 750 770 790]\n",
      "数据类型: int64\n",
      "\n",
      "列名:  holding_time\n",
      "唯一值: ['10' '30' '60' '15' '24']\n",
      "数据类型: object\n",
      "\n",
      "数值型列: ['Ni', 'Mn', 'Li', 'dapant', 'tem']\n"
     ]
    }
   ],
   "source": [
    "# 检查每列的数据类型和唯一值\n",
    "for col in df.columns:\n",
    "    print(f\"\\n列名: {col}\")\n",
    "    print(\"唯一值:\", df[col].unique()[:5])  # 显示前5个唯一值\n",
    "    print(\"数据类型:\", df[col].dtype)\n",
    "\n",
    "# 识别数值型列\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\n数值型列:\", numeric_cols.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换后的数据预览：\n",
      "[[ 95.   0.   4. 101.   0.   1. 800.  10.]\n",
      " [ 95.   0.   4. 101.  24.   1. 800.  10.]\n",
      " [ 95.   0.   4. 101.  21.   1. 800.  10.]\n",
      " [ 95.   0.   4. 101.  17.   1. 800.  10.]\n",
      " [ 95.   0.   4. 101.  16.   1. 800.  10.]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 定义函数来提取数字\n",
    "def extract_number(x):\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    # 提取字符串中的数字\n",
    "    numbers = re.findall(r'\\d+', str(x))\n",
    "    return float(numbers[0]) if numbers else 0\n",
    "\n",
    "# 对每一列应用转换\n",
    "df = df.applymap(extract_number)\n",
    "\n",
    "# 转换为numpy数组\n",
    "data = df.to_numpy()\n",
    "\n",
    "print(\"转换后的数据预览：\")\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到 processed_data1.csv\n",
      "转换后的数据预览：\n",
      "[[ 95.   0.   4. 101.   0.   1. 800.  10.]\n",
      " [ 95.   0.   4. 101.  24.   1. 800.  10.]\n",
      " [ 95.   0.   4. 101.  21.   1. 800.  10.]\n",
      " [ 95.   0.   4. 101.  17.   1. 800.  10.]\n",
      " [ 95.   0.   4. 101.  16.   1. 800.  10.]]\n"
     ]
    }
   ],
   "source": [
    "# 对每一列应用转换\n",
    "df = df.applymap(extract_number)\n",
    "\n",
    "# 转换为numpy数组\n",
    "data = df.to_numpy()\n",
    "\n",
    "# 将数据保存为CSV文件\n",
    "import pandas as pd\n",
    "pd.DataFrame(data).to_csv('processed_data1.csv', index=False)\n",
    "print(\"数据已保存到 processed_data1.csv\")\n",
    "\n",
    "print(\"转换后的数据预览：\")\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground_truth = scale(data) # \"wine\" can be replaced with any of the datasets\n",
    "                                             # supported by dataset_loader (see data_loaders.py)\n",
    "#ground_truth = data\n",
    "#X_true = torch.from_numpy(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_scaler import DataScaler\n",
    "scaler = DataScaler()\n",
    "ground_truth = scaler.fit_transform(data)\n",
    "\n",
    "X_true = torch.from_numpy(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISS DATA GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "p = 0.3\n",
    "\n",
    "mask = np.random.rand(*ground_truth.shape) < p #True for missing values, false for others\n",
    "x_miss = np.copy(ground_truth)\n",
    "\n",
    "x_miss[mask] = np.nan\n",
    "X_miss = torch.from_numpy(x_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = X_miss.shape\n",
    "batchsize = 128 # If the batch size is larger than half the dataset's size,\n",
    "                # it will be redefined in the imputation methods.\n",
    "lr = 1e-2\n",
    "epsilon = pick_epsilon(X_miss) # Set the regularization parameter as a multiple of the median distance, as per the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # sinkhorn algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_imputer = OTimputer(eps=epsilon, batchsize=batchsize, lr=lr, niter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:batchsize = 128, epsilon = 0.1659\n",
      "INFO:root:Iteration 0:\t Loss: 1.3389\t Validation MAE: 0.7291\tRMSE: 1.0513\n",
      "INFO:root:Iteration 500:\t Loss: 0.8614\t Validation MAE: 0.5603\tRMSE: 0.9898\n",
      "INFO:root:Iteration 1000:\t Loss: 0.6857\t Validation MAE: 0.5700\tRMSE: 0.9999\n",
      "INFO:root:Iteration 1500:\t Loss: 0.9164\t Validation MAE: 0.5679\tRMSE: 1.0031\n"
     ]
    }
   ],
   "source": [
    "sk_imp, sk_maes, sk_rmses = sk_imputer.fit_transform(X_miss, verbose=True, report_interval=500, X_true=X_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(sk_maes.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.813477974126047\n",
      "RMSE: 18.152475443448097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# 转换为DataFrame\n",
    "restored_data = scaler.inverse_transform(sk_imp.detach_().numpy())\n",
    "#MAE = mean_absolute_error(data, restored_data.detach().numpy())\n",
    "\n",
    "mae = mean_absolute_error(data, restored_data)\n",
    "\n",
    "mse = mean_squared_error(data, restored_data)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到 imputed_data.csv\n"
     ]
    }
   ],
   "source": [
    "df_imp = pd.DataFrame(restored_data)  # 先detach再转换为numpy数组\n",
    "\n",
    "# 保存为CSV文件\n",
    "df_imp.to_csv('imputed_data1.csv', index=False)\n",
    "print(\"数据已保存到 imputed_data1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7354394864939366\n",
      "21.336747406738393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\sklearn\\impute\\_iterative.py:686: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# 设置MICE插补器\n",
    "imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "\n",
    "# 对缺失值进行插补\n",
    "x_imputed = imputer.fit_transform(x_miss)\n",
    "\n",
    "restored_data = scaler.inverse_transform(x_imputed)\n",
    "MAE = mean_absolute_error(data, restored_data)\n",
    "mse = mean_squared_error(data, restored_data)\n",
    "rmse = np.sqrt(mse)\n",
    "#MAE = mean_absolute_error(sk_imp.detach().numpy(), x_imputed)\n",
    "print(MAE)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "\n",
    "# 设置MICE插补器\n",
    "imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "\n",
    "# 对缺失值进行插补\n",
    "x_imputed = imputer.fit_transform(x_miss)\n",
    "\n",
    "# 计算 MAE 和 RMSE\n",
    "mask = np.isnan(x_miss)  # 获取缺失值的位置\n",
    "mae = np.mean(np.abs(x_imputed[mask] - ground_truth[mask]))  # 计算MAE\n",
    "rmse = np.sqrt(np.mean((x_imputed[mask] - ground_truth[mask])**2))  # 计算RMSE\n",
    "\n",
    "print(\"验证 MAE:\", mae)\n",
    "print(\"验证 RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liner round-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the imputation models\n",
    "d_ = d - 1\n",
    "models = {}\n",
    "\n",
    "for i in range(d):\n",
    "    models[i] = nn.Linear(d_, 1)\n",
    "\n",
    "#Create the imputer\n",
    "lin_rr_imputer = RRimputer(models, eps=epsilon, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_imp, lin_maes, lin_rmses = lin_rr_imputer.fit_transform(X_miss, verbose=True, X_true=X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## MLP round-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the imputation models\n",
    "d_ = d - 1\n",
    "models = {}\n",
    "\n",
    "for i in range(d):\n",
    "    models[i] = nn.Sequential(nn.Linear(d_, 2 * d_),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(2 * d_, d_),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(d_, 1))\n",
    "\n",
    "#Create the imputer\n",
    "mlp_rr_imputer = RRimputer(models, eps=epsilon, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_imp, mlp_maes, mlp_rmses = mlp_rr_imputer.fit_transform(X_miss, verbose=True, X_true=X_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## column miss data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "\n",
    "n_samples, n_features = ground_truth.shape\n",
    "total_missing = int(0.3 * n_samples * n_features)\n",
    "\n",
    "# 随机打乱列的顺序并选择足够数量的列\n",
    "cols = np.random.permutation(n_features)\n",
    "selected_cols = []\n",
    "candidate_count = 0\n",
    "\n",
    "# 选择列直到候选元素足够覆盖总缺失数\n",
    "for col in cols:\n",
    "    selected_cols.append(col)\n",
    "    candidate_count += n_samples\n",
    "    if candidate_count >= total_missing:\n",
    "        break\n",
    "\n",
    "# 生成所有候选位置的索引\n",
    "rows = np.arange(n_samples)\n",
    "candidate_rows, candidate_cols = np.meshgrid(rows, selected_cols)\n",
    "candidate_rows = candidate_rows.ravel()\n",
    "candidate_cols = candidate_cols.ravel()\n",
    "\n",
    "# 随机打乱并选择指定数量的缺失位置\n",
    "indices = np.random.permutation(len(candidate_rows))[:total_missing]\n",
    "missing_rows = candidate_rows[indices]\n",
    "missing_cols = candidate_cols[indices]\n",
    "\n",
    "# 创建缺失数据\n",
    "x_miss = np.copy(ground_truth)\n",
    "x_miss[missing_rows, missing_cols] = np.nan\n",
    "\n",
    "X_miss = torch.from_numpy(x_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置MICE插补器\n",
    "imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "\n",
    "# 对缺失值进行插补\n",
    "x_imputed = imputer.fit_transform(x_miss)\n",
    "\n",
    "# 计算 MAE 和 RMSE\n",
    "mask = np.isnan(x_miss)  # 获取缺失值的位置\n",
    "mae = np.mean(np.abs(x_imputed[mask] - ground_truth[mask]))  # 计算MAE\n",
    "rmse = np.sqrt(np.mean((x_imputed[mask] - ground_truth[mask])**2))  # 计算RMSE\n",
    "print(\"验证 MAE:\", mae)\n",
    "print(\"验证 RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_imputer = OTimputer(eps=epsilon, batchsize=batchsize, lr=lr, niter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_imp, sk_maes, sk_rmses = sk_imputer.fit_transform(X_miss, verbose=True, report_interval=500, X_true=X_true)\n",
    "#sinkhorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the imputation models\n",
    "d_ = d - 1\n",
    "models = {}\n",
    "\n",
    "for i in range(d):\n",
    "    models[i] = nn.Linear(d_, 1)\n",
    "\n",
    "#Create the imputer\n",
    "lin_rr_imputer = RRimputer(models, eps=epsilon, lr=lr)\n",
    "lin_imp, lin_maes, lin_rmses = lin_rr_imputer.fit_transform(X_miss, verbose=True, X_true=X_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
